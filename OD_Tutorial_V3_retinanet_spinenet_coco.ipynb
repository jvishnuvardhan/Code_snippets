{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jvishnuvardhan/Code_snippets/blob/master/OD_Tutorial_V3_retinanet_spinenet_coco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16pSAZA7QQ9g"
      },
      "outputs": [],
      "source": [
        "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npuyt-VBQWfg"
      },
      "source": [
        "# Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSjfPtjGWk4s",
        "outputId": "042df2eb-b5b7-4c3b-8214-228ca9dea4b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: opencv-python 4.6.0.66\n",
            "Uninstalling opencv-python-4.6.0.66:\n",
            "  Successfully uninstalled opencv-python-4.6.0.66\n",
            "\u001b[K     |████████████████████████████████| 578.0 MB 16 kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 47.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 438 kB 36.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 56.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 55.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 40.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 238 kB 57.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 50.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 116 kB 35.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 9.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 38.2 MB 179 kB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 39.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 47.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 52.5 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python-headless==4.5.2.52 in /usr/local/lib/python3.7/dist-packages (4.5.2.52)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless==4.5.2.52) (1.21.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ]
        }
      ],
      "source": [
        "# Import main libraries\n",
        "!pip uninstall -y opencv-python\n",
        "!pip install -U -q \"tensorflow>=2.9.0\" \"tf-models-official\"\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "!pip install opencv-python-headless==4.5.2.52\n",
        "import tensorflow_models as tfm\n",
        "from official.vision.ops.preprocess_ops import normalize_image, resize_and_crop_image\n",
        "# import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Cd8axr7eJvv"
      },
      "outputs": [],
      "source": [
        "# Import helper libraries\n",
        "import pprint\n",
        "import tempfile\n",
        "from IPython import display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import io\n",
        "import pathlib\n",
        "import cv2\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from six.moves.urllib.request import urlopen\n",
        "from pprint import pprint # for printing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2qOHSn1FVoF"
      },
      "source": [
        "## Visualization tools\n",
        "\n",
        "To visualize the images with the proper detected boxes, we will use the TensorFlow Object Detection API. To install it we will clone the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-zIfq3W--P1",
        "outputId": "62f9ef64-9695-49b4-9163-2c00e26cc1dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 3483, done.\u001b[K\n",
            "remote: Counting objects: 100% (3483/3483), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2912/2912), done.\u001b[K\n",
            "remote: Total 3483 (delta 907), reused 1491 (delta 518), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3483/3483), 46.91 MiB | 20.62 MiB/s, done.\n",
            "Resolving deltas: 100% (907/907), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone the tensorflow models repository\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmb1DpCe_Arc",
        "outputId": "a5b5d50b-3584-4bfc-9260-22a793372b01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "protobuf-compiler is already the newest version (3.0.0-9.1ubuntu1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/models/research\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.41.0-cp37-cp37m-manylinux2010_x86_64.whl (10.9 MB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.9.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.32)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.4)\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.7.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.3.5)\n",
            "Requirement already satisfied: tf-models-official>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.10.0)\n",
            "Collecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.27.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.0 MB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.10.0)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "Requirement already satisfied: sacrebleu<=2.2.0 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (1.21.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.4.5)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2.5.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2022.6.2)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.2.1)\n",
            "Requirement already satisfied: opencv-python-headless==4.5.2.52 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.5.2.52)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.7.3)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.6.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-text~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.10.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.1.97)\n",
            "Requirement already satisfied: tensorflow~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.10.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.11)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.2.2)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (8.0.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.17.1)\n",
            "Requirement already satisfied: pyyaml<6.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.1)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.35.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.31.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.56.4)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.0)\n",
            "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.17.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2022.2.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (6.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2022.6.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.64.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (14.0.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (2.0.7)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.48.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (0.26.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (4.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (2.10.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (3.4.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (3.8.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.7)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Requirement already satisfied: pyarrow<8.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (6.0.1)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n",
            "Collecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.6.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "Collecting proto-plus<2,>=1.7.1\n",
            "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
            "Collecting requests<3.0.0dev,>=2.18.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "Collecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "Collecting cloudpickle<3,>=2.1.0\n",
            "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting protobuf<4.0.0dev,>=3.12.0\n",
            "  Downloading protobuf-3.19.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.4.4)\n",
            "Collecting opencv-python>=4.1.0.25\n",
            "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.9.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.7.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.10.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.10.2)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "Building wheels for collected packages: object-detection, dill, avro-python3, docopt\n",
            "  Building wheel for object-detection (setup.py): started\n",
            "  Building wheel for object-detection (setup.py): finished with status 'done'\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1696118 sha256=b2d6b54ee227de70d17173a0dca4aaa242a7c8f048be37c64ec7160bd0076ae1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ooo6ty1m/wheels/fa/a4/d2/e9a5057e414fd46c8e543d2706cd836d64e1fcd9eccceb2329\n",
            "  Building wheel for dill (setup.py): started\n",
            "  Building wheel for dill (setup.py): finished with status 'done'\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=4bd9cd42b7fabe89971f89edc195408b60293c50bbe60d22034f60bfa416efc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for avro-python3 (setup.py): started\n",
            "  Building wheel for avro-python3 (setup.py): finished with status 'done'\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44010 sha256=7e094f37c61ad7a056b4d94ec7741c994948a903786b3f36ba7bfd3fff034e7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/e5/b1/6b151d9b535ee50aaa6ab27d145a0104b6df02e5636f0376da\n",
            "  Building wheel for docopt (setup.py): started\n",
            "  Building wheel for docopt (setup.py): finished with status 'done'\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=5a7bb49cc4fdbd604532b79e641c6c57f8af8d3cf4eed2eabefda3408394d63e\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built object-detection dill avro-python3 docopt\n",
            "Installing collected packages: requests, pyparsing, protobuf, tensorflow-io-gcs-filesystem, docopt, dill, pymongo, proto-plus, orjson, opencv-python, hdfs, fastavro, cloudpickle, tensorflow-io, lvis, avro-python3, apache-beam, object-detection\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.26.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.26.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.26.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.5.1\n",
            "    Uninstalling dill-0.3.5.1:\n",
            "      Successfully uninstalled dill-0.3.5.1\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.2.0\n",
            "    Uninstalling pymongo-4.2.0:\n",
            "      Successfully uninstalled pymongo-4.2.0\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.5.0\n",
            "    Uninstalling cloudpickle-1.5.0:\n",
            "      Successfully uninstalled cloudpickle-1.5.0\n",
            "Successfully installed apache-beam-2.41.0 avro-python3-1.10.2 cloudpickle-2.2.0 dill-0.3.1.1 docopt-0.6.2 fastavro-1.6.1 hdfs-2.7.0 lvis-0.5.3 object-detection-0.1 opencv-python-4.6.0.66 orjson-3.8.0 proto-plus-1.22.1 protobuf-3.19.5 pymongo-3.12.3 pyparsing-2.4.7 requests-2.28.1 tensorflow-io-0.27.0 tensorflow-io-gcs-filesystem-0.27.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "sudo apt install -y protobuf-compiler\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm7jeDe-_DnL"
      },
      "outputs": [],
      "source": [
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2AnOGUNZXU4"
      },
      "source": [
        "## Utilities\n",
        "\n",
        "Run the following cell to create some utils that will be needed later:\n",
        "\n",
        "- Helper method to load an image\n",
        "- Map of Model Name to TF Hub handle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_XZe8SvZbMO"
      },
      "outputs": [],
      "source": [
        "# Inputs to preprocess functions\n",
        "MEAN_RGB = (0.485 * 255, 0.456 * 255, 0.406 * 255)\n",
        "STDDEV_RGB = (0.229 * 255, 0.224 * 255, 0.225 * 255)\n",
        "CENTER_CROP_FRACTION = 0.875\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  image = None\n",
        "  if(path.startswith('http')):\n",
        "    response = urlopen(path)\n",
        "    image_data = response.read()\n",
        "    image_data = BytesIO(image_data)\n",
        "    image = Image.open(image_data)\n",
        "  else:\n",
        "    image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (1, im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "def inverse_normalize_image(image,\n",
        "                    offset=(0.485, 0.456, 0.406),\n",
        "                    scale=(0.229, 0.224, 0.225)):\n",
        "  \"\"\"Normalizes the image to zero mean and unit variance.\"\"\"\n",
        "  with tf.name_scope('inverse_normalize_image'):\n",
        "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    scale = tf.constant(scale)\n",
        "    scale = tf.expand_dims(scale, axis=0)\n",
        "    scale = tf.expand_dims(scale, axis=0)\n",
        "    image *= scale\n",
        "\n",
        "    offset = tf.constant(offset)\n",
        "    offset = tf.expand_dims(offset, axis=0)\n",
        "    offset = tf.expand_dims(offset, axis=0)\n",
        "    image += offset\n",
        "    return image\n",
        "\n",
        "\n",
        "def build_inputs_for_segmentation(image, input_image_size, MEAN_RGB, STDDEV_RGB):\n",
        "  \"\"\"Builds segmentation model inputs for serving.\"\"\"\n",
        "  # Normalizes image with mean and std pixel values.\n",
        "  image = normalize_image(image, offset=MEAN_RGB, scale=STDDEV_RGB)\n",
        "  image, _ = resize_and_crop_image(\n",
        "      image,\n",
        "      input_image_size,\n",
        "      padded_size=input_image_size,\n",
        "      aug_scale_min=1.0,\n",
        "      aug_scale_max=1.0)\n",
        "  return image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEKlY-dVX1oz",
        "outputId": "33011470-b45f-4ad4-e2b0-d209faf4d402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R50sDubyWuSk"
      },
      "outputs": [],
      "source": [
        "# \"\"\"Semantic segmentation configuration definition.\"\"\"\n",
        "import dataclasses\n",
        "import os\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "from official.core import exp_factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz5hAt7jczwh"
      },
      "outputs": [],
      "source": [
        "# from absl.testing import parameterized\n",
        "# from official import vision\n",
        "from official.core import config_definitions as cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhVbC--1ZQmX",
        "outputId": "2184bba1-4f62-4947-a788-cb3a48ebbe54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'common', 'core', 'modeling', 'nlp', 'projects', 'vision']\n"
          ]
        }
      ],
      "source": [
        "import official\n",
        "print(dir(official))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2xXCeo3ctTn",
        "outputId": "6d27254d-6ba8-4f11-a42f-bb841b003d2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "91\n"
          ]
        }
      ],
      "source": [
        "# Initialize experimental config\n",
        "# exp_config = tfm.core.exp_factory.get_exp_config('seg_deeplabv3_pascal')\n",
        "# exp_config = tfm.core.exp_factory.get_exp_config('retinanet_resnetfpn_coco')\n",
        "exp_config = tfm.core.exp_factory.get_exp_config('retinanet_spinenet_coco')\n",
        "# exp_config = tfm.core.exp_factory.get_exp_config('retinanet_resnetfpn_coco')\n",
        "\n",
        "# tfds_name = 'cifar10'\n",
        "print(exp_config.task.model.num_classes) # 91 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iHtNrkGcSfe",
        "outputId": "ebe0b7ea-6d66-495b-8ae3-35e7dc769a67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['task', 'trainer', 'runtime'])\n"
          ]
        }
      ],
      "source": [
        "print(exp_config.as_dict().keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7OtdR8DI5Q7"
      },
      "outputs": [],
      "source": [
        "# defining category labels\n",
        "# category_index={1: {'id': 1, 'name': 'Car'},\n",
        "#  2: {'id': 2, 'name': 'Bus'}}\n",
        "category_index={1: {'id': 1, 'name': 'Platelets'},\n",
        " 2: {'id': 2, 'name': 'RBC'},3: {'id': 3, 'name': 'WBC'}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kuq1L2uPfHtR"
      },
      "outputs": [],
      "source": [
        "# Configure model\n",
        "exp_config.task.model.num_classes = 2\n",
        "# exp_config.task.model.backbone.resnet.model_id = 18\n",
        "\n",
        "# Configure training and testing data\n",
        "batch_size = 1\n",
        "\n",
        "# exp_config.task.train_data.input_path = '/content/gdrive/MyDrive/OD_SmallSet_coco/output/train.tfrecord'\n",
        "# exp_config.task.train_data.input_path = '/content/gdrive/MyDrive/OD_MediumSet_coco/output/train.tfrecord'\n",
        "exp_config.task.train_data.input_path = '/content/gdrive/MyDrive/BCCD/train/cells.tfrecord'\n",
        "\n",
        "exp_config.task.train_data.dtype = 'float32'\n",
        "exp_config.task.train_data.global_batch_size = batch_size\n",
        "\n",
        "# exp_config.task.validation_data.input_path = '/content/gdrive/MyDrive/OD_SmallSet_coco/output/train.tfrecord'\n",
        "# exp_config.task.validation_data.input_path = '/content/gdrive/MyDrive/OD_MediumSet_coco/output/train.tfrecord'\n",
        "exp_config.task.validation_data.input_path = '/content/gdrive/MyDrive/BCCD/train/cells.tfrecord'\n",
        "exp_config.task.validation_data.dtype = 'float32'\n",
        "exp_config.task.validation_data.global_batch_size = batch_size\n",
        "\n",
        "# exp_config.task.annotation_file ='/content/gdrive/MyDrive/OD_SmallSet_coco/annotations.json'\n",
        "# exp_config.task.annotation_file ='/content/gdrive/MyDrive/OD_MediumSet_coco/train_coco_json.json'\n",
        "exp_config.task.annotation_file ='/content/gdrive/MyDrive/BCCD/train/_annotations.coco.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcfN5rBMWr53",
        "outputId": "ae5d2d0a-035a-4a70-cee1-09e55e4abcb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on CPU is slow, so only train for a few steps.\n"
          ]
        }
      ],
      "source": [
        "logical_device_names = [logical_device.name for logical_device in tf.config.list_logical_devices()]\n",
        "\n",
        "if 'GPU' in ''.join(logical_device_names):\n",
        "  print('This may be broken in Colab.')\n",
        "  device = 'GPU'\n",
        "elif 'TPU' in ''.join(logical_device_names):\n",
        "  print('This may be broken in Colab.')\n",
        "  device = 'TPU'\n",
        "else:\n",
        "  print('Running on CPU is slow, so only train for a few steps.')\n",
        "  device = 'CPU'\n",
        "\n",
        "if device=='CPU':\n",
        "  train_steps = 2 # 20\n",
        "  exp_config.trainer.steps_per_loop = 2 #5\n",
        "else:\n",
        "  train_steps=6\n",
        "  exp_config.trainer.steps_per_loop = 2\n",
        "\n",
        "exp_config.trainer.summary_interval = 100\n",
        "exp_config.trainer.checkpoint_interval = train_steps\n",
        "exp_config.trainer.validation_interval = 100\n",
        "exp_config.trainer.validation_steps =  2 #ds_info.splits['test'].num_examples // batch_size\n",
        "exp_config.trainer.train_steps = train_steps\n",
        "exp_config.trainer.optimizer_config.learning_rate.type = 'cosine'\n",
        "exp_config.trainer.optimizer_config.learning_rate.cosine.decay_steps = train_steps\n",
        "exp_config.trainer.optimizer_config.learning_rate.cosine.initial_learning_rate = 0.1\n",
        "exp_config.trainer.optimizer_config.warmup.linear.warmup_steps = 100\n",
        "exp_config.task.model.detection_generator.max_classes_per_detection = exp_config.task.model.num_classes\n",
        "exp_config.runtime.mixed_precision_dtype = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2gzOEDIepp5",
        "outputId": "1436bee6-fc9c-47c1-bcd4-9951a93859e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'runtime': {'all_reduce_alg': None,\n",
            "             'batchnorm_spatial_persistent': False,\n",
            "             'dataset_num_private_threads': None,\n",
            "             'default_shard_dim': -1,\n",
            "             'distribution_strategy': 'mirrored',\n",
            "             'enable_xla': False,\n",
            "             'gpu_thread_mode': None,\n",
            "             'loss_scale': None,\n",
            "             'mixed_precision_dtype': None,\n",
            "             'num_cores_per_replica': 1,\n",
            "             'num_gpus': 0,\n",
            "             'num_packs': 1,\n",
            "             'per_gpu_thread_count': 0,\n",
            "             'run_eagerly': False,\n",
            "             'task_index': -1,\n",
            "             'tpu': None,\n",
            "             'tpu_enable_xla_dynamic_padder': None,\n",
            "             'worker_hosts': None},\n",
            " 'task': {'annotation_file': '/content/gdrive/MyDrive/BCCD/train/_annotations.coco.json',\n",
            "          'differential_privacy_config': None,\n",
            "          'export_config': {'cast_detection_classes_to_float': False,\n",
            "                            'cast_num_detections_to_float': False,\n",
            "                            'output_normalized_coordinates': False},\n",
            "          'freeze_backbone': False,\n",
            "          'init_checkpoint': None,\n",
            "          'init_checkpoint_modules': 'all',\n",
            "          'losses': {'box_loss_weight': 50,\n",
            "                     'focal_loss_alpha': 0.25,\n",
            "                     'focal_loss_gamma': 1.5,\n",
            "                     'huber_loss_delta': 0.1,\n",
            "                     'l2_weight_decay': 4e-05,\n",
            "                     'loss_weight': 1.0},\n",
            "          'model': {'anchor': {'anchor_size': 3,\n",
            "                               'aspect_ratios': [0.5, 1.0, 2.0],\n",
            "                               'num_scales': 3},\n",
            "                    'backbone': {'spinenet': {'max_level': 7,\n",
            "                                              'min_level': 3,\n",
            "                                              'model_id': '49',\n",
            "                                              'stochastic_depth_drop_rate': 0.2},\n",
            "                                 'type': 'spinenet'},\n",
            "                    'decoder': {'identity': {}, 'type': 'identity'},\n",
            "                    'detection_generator': {'apply_nms': True,\n",
            "                                            'max_classes_per_detection': 2,\n",
            "                                            'max_num_detections': 100,\n",
            "                                            'nms_iou_threshold': 0.5,\n",
            "                                            'nms_version': 'v2',\n",
            "                                            'pre_nms_score_threshold': 0.05,\n",
            "                                            'pre_nms_top_k': 5000,\n",
            "                                            'soft_nms_sigma': None,\n",
            "                                            'tflite_post_processing': {'max_classes_per_detection': 5,\n",
            "                                                                       'max_detections': 200,\n",
            "                                                                       'nms_iou_threshold': 0.5,\n",
            "                                                                       'nms_score_threshold': 0.1,\n",
            "                                                                       'use_regular_nms': False},\n",
            "                                            'use_cpu_nms': False},\n",
            "                    'head': {'attribute_heads': [],\n",
            "                             'num_convs': 4,\n",
            "                             'num_filters': 256,\n",
            "                             'share_classification_heads': False,\n",
            "                             'use_separable_conv': False},\n",
            "                    'input_size': [640, 640, 3],\n",
            "                    'max_level': 7,\n",
            "                    'min_level': 3,\n",
            "                    'norm_activation': {'activation': 'swish',\n",
            "                                        'norm_epsilon': 0.001,\n",
            "                                        'norm_momentum': 0.99,\n",
            "                                        'use_sync_bn': True},\n",
            "                    'num_classes': 2},\n",
            "          'name': None,\n",
            "          'per_category_metrics': False,\n",
            "          'train_data': {'block_length': 1,\n",
            "                         'cache': False,\n",
            "                         'cycle_length': None,\n",
            "                         'decoder': {'simple_decoder': {'mask_binarize_threshold': None,\n",
            "                                                        'regenerate_source_id': False},\n",
            "                                     'type': 'simple_decoder'},\n",
            "                         'deterministic': None,\n",
            "                         'drop_remainder': True,\n",
            "                         'dtype': 'float32',\n",
            "                         'enable_tf_data_service': False,\n",
            "                         'file_type': 'tfrecord',\n",
            "                         'global_batch_size': 1,\n",
            "                         'input_path': '/content/gdrive/MyDrive/BCCD/train/cells.tfrecord',\n",
            "                         'is_training': True,\n",
            "                         'parser': {'aug_policy': None,\n",
            "                                    'aug_rand_hflip': True,\n",
            "                                    'aug_scale_max': 2.0,\n",
            "                                    'aug_scale_min': 0.1,\n",
            "                                    'aug_type': None,\n",
            "                                    'match_threshold': 0.5,\n",
            "                                    'max_num_instances': 100,\n",
            "                                    'num_channels': 3,\n",
            "                                    'skip_crowd_during_training': True,\n",
            "                                    'unmatched_threshold': 0.5},\n",
            "                         'prefetch_buffer_size': None,\n",
            "                         'seed': None,\n",
            "                         'sharding': True,\n",
            "                         'shuffle_buffer_size': 10000,\n",
            "                         'tf_data_service_address': None,\n",
            "                         'tf_data_service_job_name': None,\n",
            "                         'tfds_as_supervised': False,\n",
            "                         'tfds_data_dir': '',\n",
            "                         'tfds_name': '',\n",
            "                         'tfds_skip_decoding_feature': '',\n",
            "                         'tfds_split': ''},\n",
            "          'use_coco_metrics': True,\n",
            "          'use_wod_metrics': False,\n",
            "          'validation_data': {'block_length': 1,\n",
            "                              'cache': False,\n",
            "                              'cycle_length': None,\n",
            "                              'decoder': {'simple_decoder': {'mask_binarize_threshold': None,\n",
            "                                                             'regenerate_source_id': False},\n",
            "                                          'type': 'simple_decoder'},\n",
            "                              'deterministic': None,\n",
            "                              'drop_remainder': True,\n",
            "                              'dtype': 'float32',\n",
            "                              'enable_tf_data_service': False,\n",
            "                              'file_type': 'tfrecord',\n",
            "                              'global_batch_size': 1,\n",
            "                              'input_path': '/content/gdrive/MyDrive/BCCD/train/cells.tfrecord',\n",
            "                              'is_training': False,\n",
            "                              'parser': {'aug_policy': None,\n",
            "                                         'aug_rand_hflip': False,\n",
            "                                         'aug_scale_max': 1.0,\n",
            "                                         'aug_scale_min': 1.0,\n",
            "                                         'aug_type': None,\n",
            "                                         'match_threshold': 0.5,\n",
            "                                         'max_num_instances': 100,\n",
            "                                         'num_channels': 3,\n",
            "                                         'skip_crowd_during_training': True,\n",
            "                                         'unmatched_threshold': 0.5},\n",
            "                              'prefetch_buffer_size': None,\n",
            "                              'seed': None,\n",
            "                              'sharding': True,\n",
            "                              'shuffle_buffer_size': 10000,\n",
            "                              'tf_data_service_address': None,\n",
            "                              'tf_data_service_job_name': None,\n",
            "                              'tfds_as_supervised': False,\n",
            "                              'tfds_data_dir': '',\n",
            "                              'tfds_name': '',\n",
            "                              'tfds_skip_decoding_feature': '',\n",
            "                              'tfds_split': ''}},\n",
            " 'trainer': {'allow_tpu_summary': False,\n",
            "             'best_checkpoint_eval_metric': '',\n",
            "             'best_checkpoint_export_subdir': '',\n",
            "             'best_checkpoint_metric_comp': 'higher',\n",
            "             'checkpoint_interval': 2,\n",
            "             'continuous_eval_timeout': 3600,\n",
            "             'eval_tf_function': True,\n",
            "             'eval_tf_while_loop': False,\n",
            "             'loss_upper_bound': 1000000.0,\n",
            "             'max_to_keep': 5,\n",
            "             'optimizer_config': {'ema': None,\n",
            "                                  'learning_rate': {'cosine': {'alpha': 0.0,\n",
            "                                                               'decay_steps': 2,\n",
            "                                                               'initial_learning_rate': 0.1,\n",
            "                                                               'name': 'CosineDecay',\n",
            "                                                               'offset': 0},\n",
            "                                                    'type': 'cosine'},\n",
            "                                  'optimizer': {'sgd': {'clipnorm': None,\n",
            "                                                        'clipvalue': None,\n",
            "                                                        'decay': 0.0,\n",
            "                                                        'global_clipnorm': None,\n",
            "                                                        'momentum': 0.9,\n",
            "                                                        'name': 'SGD',\n",
            "                                                        'nesterov': False},\n",
            "                                                'type': 'sgd'},\n",
            "                                  'warmup': {'linear': {'name': 'linear',\n",
            "                                                        'warmup_learning_rate': 0.0067,\n",
            "                                                        'warmup_steps': 100},\n",
            "                                             'type': 'linear'}},\n",
            "             'recovery_begin_steps': 0,\n",
            "             'recovery_max_trials': 0,\n",
            "             'steps_per_loop': 2,\n",
            "             'summary_interval': 100,\n",
            "             'train_steps': 2,\n",
            "             'train_tf_function': True,\n",
            "             'train_tf_while_loop': True,\n",
            "             'validation_interval': 100,\n",
            "             'validation_steps': 2,\n",
            "             'validation_summary_subdir': 'validation'}}\n"
          ]
        }
      ],
      "source": [
        "# Check the default parameters in the experimental config\n",
        "pprint(exp_config.as_dict()) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvEgoVJyfOMp",
        "outputId": "9a19f159-7f71-416f-a041-7918f7ab2b8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: this will be really slow.\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "# Setting up the Strategy\n",
        "logical_device_names = [logical_device.name for logical_device in tf.config.list_logical_devices()]\n",
        "\n",
        "if exp_config.runtime.mixed_precision_dtype == tf.float16:\n",
        "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "if 'GPU' in ''.join(logical_device_names):\n",
        "  distribution_strategy = tf.distribute.MirroredStrategy()\n",
        "elif 'TPU' in ''.join(logical_device_names):\n",
        "  tf.tpu.experimental.initialize_tpu_system()\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='/device:TPU_SYSTEM:0')\n",
        "  distribution_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "  print('Warning: this will be really slow.')\n",
        "  distribution_strategy = tf.distribute.OneDeviceStrategy(logical_device_names[0])\n",
        "\n",
        "print(\"Done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R_od--qm084"
      },
      "outputs": [],
      "source": [
        "def show_batch(images): # labels, predictions=None\n",
        "  # plt.figure(figsize=(10, 10))\n",
        "  min = images.numpy().min()\n",
        "  max = images.numpy().max()\n",
        "  delta = max - min\n",
        "\n",
        "  for i in range(1):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow((images[i]-min) / delta)\n",
        "    # if predictions is None:\n",
        "    #   plt.title(label_info.int2str(labels[i]))\n",
        "    # else:\n",
        "    #   if labels[i] == predictions[i]:\n",
        "    #     color = 'g'\n",
        "    #   else:\n",
        "    #     color = 'r'\n",
        "    #   plt.title(label_info.int2str(predictions[i]), color=color)\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOgYMYmg6G0g"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQxjt83sPtCh"
      },
      "outputs": [],
      "source": [
        "# model = task.build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaUYoDuPP-Xl"
      },
      "outputs": [],
      "source": [
        "# out = model(feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB5KicMBgUJM",
        "outputId": "54ae5de3-2ce1-4b51-a5fd-ad75820f403e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<official.vision.tasks.retinanet.RetinaNetTask object at 0x7f0951cd3f50>\n"
          ]
        }
      ],
      "source": [
        "with distribution_strategy.scope():\n",
        "  model_dir = tempfile.mkdtemp()\n",
        "  task = tfm.core.task_factory.get_task(exp_config.task, logging_dir=model_dir)\n",
        "  print(task)\n",
        "\n",
        "# tf.keras.utils.plot_model(task.build_model(), show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chO-n53rKuHE"
      },
      "outputs": [],
      "source": [
        "# import orbit\n",
        "# strategy = tf.distribute.get_strategy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTng_vvZKlqj"
      },
      "outputs": [],
      "source": [
        "# dataset = orbit.utils.make_distributed_dataset(strategy, task.build_inputs,\n",
        "#                                                    exp_config.task.train_data)\n",
        "# iterator = iter(dataset)\n",
        "# feats, labels = next(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LsmNIEwLWrd"
      },
      "outputs": [],
      "source": [
        "# print(feats.shape)\n",
        "# print(labels.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC_Jf_t6SD67"
      },
      "outputs": [],
      "source": [
        "# print(labels['box_targets'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr0SpM_2fJsn"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(10, 10))\n",
        "# for images, labels in task.build_inputs(exp_config.task.train_data).take(1):\n",
        "#   print(f'images.shape: {str(images.shape):16}  images.dtype: {images.dtype!r}')\n",
        "#   # print(f'labels.shape: {str(labels.shape):16}  labels.dtype: {labels.dtype!r}')\n",
        "#   # print()\n",
        "#   # # print(labels)\n",
        "#   # print(labels.keys())\n",
        "#   # plt.figure(figsize=(10, 10))\n",
        "#   # # plt.imshow(images[0])\n",
        "#   show_batch(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WVe4BsMXP5V"
      },
      "outputs": [],
      "source": [
        "# labels['image_info']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGVVE9VKZNgW",
        "outputId": "563a4a3e-779c-4ee1-8069-b5efe5f98f8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:SpineNet output level 2 out of range [min_level, max_level] = [3, 7] will not be used for further processing.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.00s)\n",
            "creating index...\n",
            "index created!\n",
            "restoring or initializing model...\n",
            "initialized model.\n",
            "train | step:      0 | training until step 2...\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-d18d13cd1da2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     run_post_eval=True)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/official/core/train_lib.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(distribution_strategy, task, mode, params, model_dir, run_post_eval, save_summary, train_actions, eval_actions, trainer, controller_cls)\u001b[0m\n\u001b[1;32m    304\u001b[0m       \u001b[0mcontroller_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontroller_cls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m   )\n\u001b[0;32m--> 306\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/official/core/train_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mtrain_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0meval_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             eval_interval=params.trainer.validation_interval)\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/orbit/controller.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(self, train_steps, eval_steps, eval_interval)\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0minterval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_steps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m       \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_at_completion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m       \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/orbit/controller.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, steps, checkpoint_at_completion)\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;31m# Calculates steps to run for the next train loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m       \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_loop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_n_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m       \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/orbit/controller.py\u001b[0m in \u001b[0;36m_train_n_steps\u001b[0;34m(self, num_steps)\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_if\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshould_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mnum_steps_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0mtrain_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;31m# Verify that global_step was updated properly, then update current_step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/orbit/standard_runner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_steps)\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_loop_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nFeature: image/source_id (data type: string) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[while/body/_1/IteratorGetNext]] [Op:__inference_loop_fn_93580]"
          ]
        }
      ],
      "source": [
        "model, eval_logs = tfm.core.train_lib.run_experiment(\n",
        "    distribution_strategy=distribution_strategy,\n",
        "    task=task,\n",
        "    mode='train_and_eval',\n",
        "    params=exp_config,\n",
        "    model_dir=model_dir,\n",
        "    run_post_eval=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OJxr_83Tv89e"
      },
      "outputs": [],
      "source": [
        "# Plotting metrics\n",
        "for key, value in eval_logs.items():\n",
        "  if tf.is_tensor(value):\n",
        "    print(f'{key:20}: {value.numpy():.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xwogsrxnxpbz"
      },
      "outputs": [],
      "source": [
        "from official.vision.serving import export_saved_model_lib\n",
        "import official.core.train_lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8xPqKBM9x1sT"
      },
      "outputs": [],
      "source": [
        "# Saving and exporting the trained model\n",
        "export_saved_model_lib.export_inference_graph(\n",
        "    input_type='image_tensor',\n",
        "    batch_size=1,\n",
        "    input_image_size=[640, 640],\n",
        "    params=exp_config,\n",
        "    checkpoint_path=tf.train.latest_checkpoint(model_dir),\n",
        "    export_dir='./export/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20NJe_OMYIHF"
      },
      "source": [
        "## Visualization of an original image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m6IsUN2X7iM"
      },
      "outputs": [],
      "source": [
        "image_path= '/content/gdrive/MyDrive/OD_MediumSet_coco/images/10647071755_c864af9a57_c.jpg' # car\n",
        "# image_path= '/content/gdrive/MyDrive/OD_MediumSet_coco/images/1302119263_9da3dc23eb_c.jpg'\n",
        "image_np = load_image_into_numpy_array(image_path)\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_np[0]/255.)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSieZAROaWhw"
      },
      "outputs": [],
      "source": [
        "# defining category labels\n",
        "category_index={1: {'id': 1, 'name': 'Car'},\n",
        " 2: {'id': 2, 'name': 'Bus'}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvVM0Qaha8w2"
      },
      "outputs": [],
      "source": [
        "input_size = (640, 640)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVNb3RBqbHSu"
      },
      "outputs": [],
      "source": [
        "# apply pre-processing functions which were applied during training the model\n",
        "image_for_prediction = build_inputs_for_segmentation(np.float32(image_np[0]), input_size, MEAN_RGB, STDDEV_RGB)\n",
        "image_for_prediction = tf.expand_dims(image_for_prediction, axis=0)\n",
        "image_for_prediction.get_shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAQfwaDZbKc7"
      },
      "outputs": [],
      "source": [
        "# display pre-processed image\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_for_prediction[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmkDjZm_bTFx"
      },
      "source": [
        "## Doing the inference\n",
        "\n",
        "To do the inference we just need to call our model on the given input image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNO9zFiy3CQL"
      },
      "outputs": [],
      "source": [
        "# Importing SavedModel\n",
        "imported = tf.saved_model.load('./export/')\n",
        "model_fn = imported.signatures['serving_default']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-LH3GADbdYd"
      },
      "outputs": [],
      "source": [
        "image_for_prediction_int8 = tf.cast(image_for_prediction, dtype = tf.uint8)\n",
        "# running inference\n",
        "results = model_fn(image_for_prediction_int8)\n",
        "\n",
        "# different object detection models have additional results\n",
        "# all of them are explained in the documentation\n",
        "result = {key:value.numpy() for key,value in results.items()}\n",
        "print(result.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YALJtrVmMSp"
      },
      "outputs": [],
      "source": [
        "results['detection_scores']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJTFyB3dmY19"
      },
      "outputs": [],
      "source": [
        "np.max(image_for_prediction_int8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSymYuqdmm_e"
      },
      "outputs": [],
      "source": [
        "model_fn.output_dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLFvi58GnM0w"
      },
      "outputs": [],
      "source": [
        "results['num_detections']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxPPaE7ucGry"
      },
      "source": [
        "## Visualizing the results\n",
        "\n",
        "Here is where we will need the model to show the detection boxes from the inference step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ymy2-DJR4mt"
      },
      "outputs": [],
      "source": [
        "height = model_fn.structured_input_signature[1]['inputs'].shape[1]\n",
        "width = model_fn.structured_input_signature[1]['inputs'].shape[2]\n",
        "input_size = (height, width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUfAkaehSCQF"
      },
      "outputs": [],
      "source": [
        "print(input_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9R0X83m55kK"
      },
      "outputs": [],
      "source": [
        "height, width = 640, 640"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Wog_6rkcDwU"
      },
      "outputs": [],
      "source": [
        "# selecting parameters for visualization\n",
        "label_id_offset = 0\n",
        "min_score_thresh =0.9\n",
        "use_normalized_coordinates=True\n",
        "if use_normalized_coordinates:\n",
        "  # Normalizing detection boxes\n",
        "  result['detection_boxes'][0][:,[0,2]] /= height\n",
        "  result['detection_boxes'][0][:,[1,3]] /= width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmFVXg5Zcoeg"
      },
      "outputs": [],
      "source": [
        "image_np_resize = cv2.resize(image_np[0], input_size[::-1], interpolation = cv2.INTER_AREA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjUqEBqxcpZ2"
      },
      "outputs": [],
      "source": [
        "# Visualizing detection boxes with scores\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_for_prediction_int8,#.astype(np.uint8), #new_image[0] image_np_resize\n",
        "      result['detection_boxes'][0],\n",
        "      (result['detection_classes'][0] + label_id_offset).astype(int),\n",
        "      result['detection_scores'][0],\n",
        "      category_index=category_index,\n",
        "      use_normalized_coordinates=use_normalized_coordinates,\n",
        "      max_boxes_to_draw=200,\n",
        "      min_score_thresh=.9,\n",
        "      agnostic_mode=False,\n",
        "      instance_masks=None, #result['detection_masks_reframed'],#.get('detection_masks_reframed', None),\n",
        "      line_thickness=2)\n",
        "\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_for_prediction_int8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1fuyO0G_MFn"
      },
      "outputs": [],
      "source": [
        "result['detection_classes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH6NaHGz6PQT"
      },
      "outputs": [],
      "source": [
        "result['detection_boxes'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP5cZaDIl1o2"
      },
      "outputs": [],
      "source": [
        "result['detection_scores'][0]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyPTC5ZQ5T/gHfVgblwQXgDH",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}